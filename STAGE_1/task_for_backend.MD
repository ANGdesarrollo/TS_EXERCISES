# ğŸ§  Etapa 1: Dominando la asincronÃ­a y gestiÃ³n de archivos como un Senior

El objetivo de esta etapa es **adquirir criterio tÃ©cnico real para enfrentar escenarios asincrÃ³nicos del mundo real**. No se trata solo de aprender `async/await`, streams o callbacks, sino de **entender cuÃ¡ndo usar cada enfoque, por quÃ©, y cÃ³mo estructurarlo para que escale, sea mantenible y robusto**.

Al finalizar esta etapa vas a poder:
- Tomar decisiones informadas entre `buffer` vs `stream`.
- Implementar control de concurrencia eficiente.
- Crear pipelines asincrÃ³nicos que manejan errores y progresos correctamente.
- Anticiparte a problemas de rendimiento y escalabilidad.
- Elevar tu nivel profesional resolviendo problemas con mentalidad senior.

ğŸ“Œ *Esta etapa estÃ¡ pensada para ejercitar la cabeza, no solo los dedos.*

---

## ğŸ“‚ Ejercicio 1: Descarga bÃ¡sica con buffer y manejo de HTTP

### ğŸ¯ Objetivo
Descargar un Ãºnico archivo desde una URL concreta y guardar localmente, gestionando cabeceras y redirecciones.

### âœï¸ Instrucciones
1. Crea un CLI `my-downloader` que reciba:
    * URL de ejemplo: `https://nbg1-speed.hetzner.com/100MB.bin`
    * Ruta destino (opcional, por defecto `./downloads`).
2. Usa `node-fetch` o `axios` para:
    * Seguir redirecciones (`maxRedirects`).
3. Descarga el contenido con `.arrayBuffer()` o `.buffer()`.
4. Guarda con `fs.writeFile`, manejando errores de red y disco.

### ğŸ” Puntos a cubrir
* Devuelve cÃ³digo de salida â‰  0 en caso de fallo.

### â“ ReflexiÃ³n
Â¿Por quÃ© elegiste buffer en lugar de stream aquÃ­? Â¿QuÃ© ventajas y desventajas ves?

---

## ğŸ“‚ Ejercicio 2: Descarga paralela y gestiÃ³n de errores

### ğŸ¯ Objetivo
Ampliar el CLI para descargar **varios** archivos en paralelo y controlar fallos individuales.

### âœï¸ Instrucciones
1. Define tu propia lista de **al menos 10 URLs** (puedes usar `speed.hetzner.de`: `10MB.bin`, `20MB.bin`, â€¦).
2. Lanza todas las descargas usando `Promise.allSettled`.
3. Si alguna falla, registra el error pero continÃºa con las demÃ¡s.
4. Devuelve al final un resumen:
    * Total exitosas vs. fallidas.
    * Tiempos individuales y global.

### ğŸ” Puntos a cubrir
* Â¿CÃ³mo manejas un timeout o un servidor caÃ­do?
* Â¿QuÃ© estrategia de retry implementarÃ­as?

### â“ ReflexiÃ³n
Si una de las descargas se retrasa mucho, Â¿cÃ³mo lo detectas y quÃ© decides hacer?

---

## ğŸ“‚ Ejercicio 2.5: Backpressure y streams

### ğŸ¯ Objetivo
Introducir el concepto de **backpressure** al descargar por stream.

### âœï¸ Instrucciones
1. Reescribe el Ejercicio 1 usando `response.body.getReader()` o `.pipe()` con `createWriteStream`.
2. Observa cÃ³mo Node.js regula el flujo de datos para no saturar la memoria.
3. Mide la memoria consumida al descargar un archivo grande y explica quÃ© ocurre si comentas `.pipe()`.

### ğŸ” Puntos a cubrir
* Â¿QuÃ© ocurre cuando la escritura en disco es mÃ¡s lenta que la descarga?
* Â¿CÃ³mo actÃºa internamente el motor de streams de Node?

### â“ ReflexiÃ³n
Â¿Por quÃ© es crÃ­tico entender backpressure en aplicaciones de alto rendimiento?

---

## ğŸ“‚ Ejercicio 3: Control de concurrencia y rate limiting

### ğŸ¯ Objetivo
Descargar 100 recursos, pero **limitando** a 5 descargas simultÃ¡neas para respetar lÃ­mites de API y consumo de recursos.

### âœï¸ Instrucciones
1. Simula 100 URLs de imÃ¡genes de ejemplo: `https://api.example.com/photos/1.jpg` â€¦ `/100.jpg`
2. Implementa una cola con `p-limit` o tu propio contador.
3. Registra la latencia y comprueba que nunca hay mÃ¡s de 5 descargas concurrentes.
4. AÃ±ade un retraso fijo (p.ej. 200 ms) entre cada descarga para simular rate limiting de la API.

### ğŸ” Puntos a cubrir
* Â¿CÃ³mo varÃ­a el tiempo total si quitas el lÃ­mite?
* Â¿QuÃ© pasa si la API retorna HTTP 429?

### â“ ReflexiÃ³n
Â¿En quÃ© escenarios reales te interesarÃ­a esta tÃ©cnica? (p.ej. calls a servicios con cuotas)

---

## ğŸš€ Ejercicio 4 (final): Pipeline completo y reporting

### ğŸ¯ Objetivo
Unir todo lo aprendido en un **caso real**: descarga, streams, concurrencia, errores y autenticaciÃ³n.

### âœï¸ Instrucciones
1. Lee un archivo `config.json` con:
    * Lista de URLs.
    * Credenciales para Basic Auth.
    * LÃ­mite de concurrencia.
    * Ruta de salida.
2. Por cada URL:
    * Descarga con streams y backpressure.
    * Respeta concurrencia y reintentos (hasta 3 veces).
    * Guarda logs en `report.csv` con: URL, estado, tamaÃ±o descargado, tiempo, intentos.
3. Al finalizar, muestra por consola estadÃ­sticas (total, Ã©xito/fallo, promedio de tiempo).

### ğŸ” Puntos a cubrir
* DiseÃ±o modular: cada responsabilidad en su mÃ³dulo.
* Limpieza en caso de cancelaciÃ³n o errores graves.

### â“ ReflexiÃ³n
Al mirar tu `report.csv`, Â¿quÃ© mÃ©tricas revisarÃ­as para optimizar aÃºn mÃ¡s este pipeline?

---

## ğŸ“š Recursos Ãºtiles
- [`node-fetch`](https://www.npmjs.com/package/node-fetch)
- [`axios`](https://axios-http.com/)
- [`fs/promises`](https://nodejs.org/api/fs.html#fspromises)
- [`p-limit`](https://www.npmjs.com/package/p-limit)
- [Control de concurrencia en JS](https://dev.to/ycmjason/javascript-concurrency-control-with-async-await-2mik)
- [Backpressure en Node.js](https://nodejs.org/en/docs/guides/backpressuring-in-streams/)
- [HTTP Status Codes](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status)
